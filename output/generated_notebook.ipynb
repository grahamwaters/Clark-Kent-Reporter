{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532d2fba",
   "metadata": {},
   "source": [
    "Data Cleaning and Preprocessing for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e3b6a",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7685a",
   "metadata": {},
   "source": [
    "Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c7e8f",
   "metadata": {},
   "source": [
    "are there any authors that are in both dataframes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d9b19",
   "metadata": {},
   "source": [
    "drop author_flair_type and author_fullname columns from both dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500d625",
   "metadata": {},
   "source": [
    "combine the title and self text columns into one column with the format `title - selftext`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015dada4",
   "metadata": {},
   "source": [
    "drop the title and selftext columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b665d56",
   "metadata": {},
   "source": [
    "rename the `title_selftext` column to `selftext`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344f4e4",
   "metadata": {},
   "source": [
    "remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98916526",
   "metadata": {},
   "source": [
    "remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84e209",
   "metadata": {},
   "source": [
    "remove whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1bf9d",
   "metadata": {},
   "source": [
    "do the same for the autism dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5525caa",
   "metadata": {},
   "source": [
    "remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c55c92",
   "metadata": {},
   "source": [
    "remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01383149",
   "metadata": {},
   "source": [
    "remove whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e613672",
   "metadata": {},
   "source": [
    "remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4361a25",
   "metadata": {},
   "source": [
    "then remove double spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207cc48",
   "metadata": {},
   "source": [
    "make a new dataframe called df_reddit that combines the two dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48e384",
   "metadata": {},
   "source": [
    "Sample Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfede8a",
   "metadata": {},
   "source": [
    "randomly sample one post from each dataframe and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2fce6",
   "metadata": {},
   "source": [
    "randomly sample one post from each dataframe and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47965e0f",
   "metadata": {},
   "source": [
    "what is the length of the shorter dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1434a4f",
   "metadata": {},
   "source": [
    "add the shorter dataframe to the new dataframe using concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00197d35",
   "metadata": {},
   "source": [
    "shorten the longer dataframe to the length of the shorter dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d568e1",
   "metadata": {},
   "source": [
    "add the shortened longer dataframe to the new dataframe using concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a8992",
   "metadata": {},
   "source": [
    "reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da3d3b",
   "metadata": {},
   "source": [
    "shuffle the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fab6d3",
   "metadata": {},
   "source": [
    "check the dimensions of the new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbdf8c8",
   "metadata": {},
   "source": [
    "double check that the number of posts for each subreddit is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2654e",
   "metadata": {},
   "source": [
    "find any of the medications in the selftext column that are in the data/drug_info.csv file under the Medication Name column and replace them with ' ' (empty string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb6c2a",
   "metadata": {},
   "source": [
    "create a list of the medications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca1782",
   "metadata": {},
   "source": [
    "how many posts contain a medication?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ba803",
   "metadata": {},
   "source": [
    "create a list of rows and the medications mentioned in each row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d153ac",
   "metadata": {},
   "source": [
    "remove the words from the selftext column that are in the medications list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad7a39",
   "metadata": {},
   "source": [
    "if the file does not already exist, create it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a298028",
   "metadata": {},
   "source": [
    "Now we want to clean the text in the self text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4ac4b",
   "metadata": {},
   "source": [
    "remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1868e8",
   "metadata": {},
   "source": [
    "remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b550e11",
   "metadata": {},
   "source": [
    "remove double spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233ab90",
   "metadata": {},
   "source": [
    "remove single characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1395aea",
   "metadata": {},
   "source": [
    "remove newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfdf10a",
   "metadata": {},
   "source": [
    "remove urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d3c20",
   "metadata": {},
   "source": [
    "remove html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9185fa",
   "metadata": {},
   "source": [
    "remove extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584c804",
   "metadata": {},
   "source": [
    "remove extra spaces at the beginning of the string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47414ea0",
   "metadata": {},
   "source": [
    "remove extra spaces at the end of the string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056b89",
   "metadata": {},
   "source": [
    "save progress to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9df15",
   "metadata": {},
   "source": [
    "read the file into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad075837",
   "metadata": {},
   "source": [
    "remove any rows that have a null value in the selftext column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c475ca",
   "metadata": {},
   "source": [
    "reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b518aa",
   "metadata": {},
   "source": [
    "check the dimensions of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f586",
   "metadata": {},
   "source": [
    "save df_reddit to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c07b4c",
   "metadata": {},
   "source": [
    "Examining UTC range based on data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822677a9",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a52a24",
   "metadata": {},
   "source": [
    "Create a new column that is the length of the selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482bb2d",
   "metadata": {},
   "source": [
    "Create a new column that is the number of words in the selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c106a",
   "metadata": {},
   "source": [
    "A column for each letter of the alphabet that is the number of times that letter appears in the selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e334e7",
   "metadata": {},
   "source": [
    "save the data to a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9152f2f",
   "metadata": {},
   "source": [
    "make a figure plotting letters against number of occurances in selftext for each selftext length bin. To avoid the ValueError \"ValueError: num must be 1 <= num <= 16, not 17\" the number of bins is set to 25 instead of 26 (the number of letters in the alphabet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0716afa",
   "metadata": {},
   "source": [
    "add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421496b",
   "metadata": {},
   "source": [
    "for this code block ignore the IndexError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97b49e",
   "metadata": {},
   "source": [
    "the suptitle should not have so much space between it and the subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e5b65",
   "metadata": {},
   "source": [
    "the x and y labels should be larger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad031650",
   "metadata": {},
   "source": [
    "if the data/cleaned_reddit_withsentiment.csv file does not exist, run the following code to create it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecc77f",
   "metadata": {},
   "source": [
    "use alivebar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39530a3",
   "metadata": {},
   "source": [
    "if the file already exists, skip this code block and load the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e7fe0",
   "metadata": {},
   "source": [
    "if the file exists but the length of the rows is not the same as the length of the rows in the cleaned_reddit.csv file, run the following code to create it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e67262",
   "metadata": {},
   "source": [
    "Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4802e1f",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb847df9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffaa29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "top_authors_ocd.head(2)\n",
    "\n",
    "```\n",
    "\n",
    "Userur      96\n",
    "corinaah    44\n",
    "Name: author, dtype: int64\n",
    "\n",
    "\n",
    "```python\n",
    "top_authors_autism.head(2)\n",
    "\n",
    "```\n",
    "\n",
    "Jupiter642           47\n",
    "anonaskingaccount    32\n",
    "Name: author, dtype: int64\n",
    "\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264645b",
   "metadata": {},
   "source": [
    "df_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "df_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41618d79",
   "metadata": {},
   "source": [
    "df_ocd = df_ocd.drop(columns=['title', 'selftext'])\n",
    "df_autism = df_autism.drop(columns=['title', 'selftext'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f9acb",
   "metadata": {},
   "source": [
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f7307",
   "metadata": {},
   "source": [
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2e74b",
   "metadata": {},
   "source": [
    "df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cccd2",
   "metadata": {},
   "source": [
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd2d26",
   "metadata": {},
   "source": [
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f731c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acce18",
   "metadata": {},
   "source": [
    "print(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\n",
    "print('='*100)\n",
    "print(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bad0db",
   "metadata": {},
   "source": [
    "if len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n",
    "    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n",
    "    longer_df = df_autism # set the longer dataframe to the Autism dataframe\n",
    "else: # if the Autism dataframe is shorter\n",
    "    shorter_df = df_autism\n",
    "    longer_df = df_ocd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af85573",
   "metadata": {},
   "source": [
    "longer_df = longer_df.head(len(shorter_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d8e53",
   "metadata": {},
   "source": [
    "df_reddit = df_reddit.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f80a38",
   "metadata": {},
   "source": [
    "print(f'Dimensions of the new dataframe: {df_reddit.shape}')\n",
    "df_reddit.head(5)\n",
    "\n",
    "```\n",
    "\n",
    "                 author created_utc      id is_original_content target  \\\n",
    "0         sharpasasquid  1587673229  g6u3ah               False      1   \n",
    "1           ColumbiaOCD  1565803842  cqcx35               False      1   \n",
    "2  Whydidideletemyaccou  1548741261  akx1ki               False      0   \n",
    "3              RexRaids  1573343825  du3xjm               False      1   \n",
    "4            Jupiter642  1562815661  cbqoe3               False      0   \n",
    "\n",
    "                                            selftext  \n",
    "0  my first ever post how is everyone hi this is ...  \n",
    "1  brain imaging for individuals withand siblings...  \n",
    "2  anyone have an article discussing possible lin...  \n",
    "3  nsfw male masturbation hi im a man and i have ...  \n",
    "4  im not able to cook although i should be becau...  \n",
    "\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b6c33",
   "metadata": {},
   "source": [
    "drug_info = pd.read_csv('../data/drug_info.csv')\n",
    "drug_info['Medication Name'] = drug_info['Medication Name'].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c88f8",
   "metadata": {},
   "source": [
    "print(f'Number of posts that contain a medication: {len(df_reddit[df_reddit.selftext.str.contains(\"|\".join(medications), regex=True)])}')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "medications[0]\n",
    "\n",
    "```\n",
    "\n",
    "'abacavir sulfate'\n",
    "\n",
    "\n",
    "```python\n",
    "medications = [med for med in medications if len(med) > 5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962c984",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654849f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0199670",
   "metadata": {},
   "source": [
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\d+', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36994d0e",
   "metadata": {},
   "source": [
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cef6a",
   "metadata": {},
   "source": [
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e2305",
   "metadata": {},
   "source": [
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b68f29",
   "metadata": {},
   "source": [
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b62b9",
   "metadata": {},
   "source": [
    "df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc101e",
   "metadata": {},
   "source": [
    "df_reddit = df_reddit.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870509e5",
   "metadata": {},
   "source": [
    "print('Saving the dataframe to a csv file')\n",
    "df_reddit.to_csv('../data/cleaned_reddit_before_dataviz.csv', index=False)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85bd16",
   "metadata": {},
   "source": [
    "df = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "try:\n",
    "    #~ Dropping Constant Value columns from the data ~#\n",
    "    df.drop(columns=['is_original_content'], inplace=True)\n",
    "    #~ Dropping duplicated selftext rows from the data ~#\n",
    "    print(f'Before dropping duplicates, the shape of the data is: {df.shape}')\n",
    "    preshape = df.shape[0]\n",
    "    df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "    print(f'After dropping duplicates, the shape of the data is: {df.shape}')\n",
    "    print(f'The number of rows dropped is: {preshape - df.shape[0]}')\n",
    "except Exception as e:\n",
    "    print(f'{e} - No duplicates to drop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9609c51",
   "metadata": {},
   "source": [
    "df['selftext_word_count'] = df['selftext'].str.split().str.len()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3929a4",
   "metadata": {},
   "source": [
    "df.to_csv('../data/cleaned_reddit.csv', index=False)\n",
    "df.head()\n",
    "\n",
    "```\n",
    "\n",
    "            author  created_utc      id  target  \\\n",
    "0            sh115   1557323366  bm5i03       1   \n",
    "1  Dazzling_Alight   1581623251  f3fnjt       1   \n",
    "2       DH2007able   1560967246  c2kcut       1   \n",
    "3          sarsapa   1551549663  awkdeg       1   \n",
    "4        nealacrea   1563397295  ceisgc       1   \n",
    "\n",
    "                                            selftext  selftext_length  \\\n",
    "0  Compulsively reading about things that scare y...             2505   \n",
    "1  Coronavirus is triggering me so badly live in ...             1013   \n",
    "2  feel like my mind just doesnt want me to be re...             1183   \n",
    "3  making mockery out of mental illness opened up...             1066   \n",
    "4  How can better communicate the extent to which...              956   \n",
    "\n",
    "   selftext_word_count    a   b   c  ...  q    r    s    t   u   v   w   x  \\\n",
    "0                  455  170  37  43  ...  1  114  147  194  61  27  30  12   \n",
    "1                  180   55  11  22  ...  0   39   44   65  28  16  12   1   \n",
    "2                  219   61  20  32  ...  0   39   65  107  41   5  17   3   \n",
    "3                  208   59   8  22  ...  1   32   57   77  21  12  22   1   \n",
    "4                  163   60   7  21  ...  0   38   51   94  19  13  13   4   \n",
    "\n",
    "    y  z  \n",
    "0  41  2  \n",
    "1  24  0  \n",
    "2  23  0  \n",
    "3  20  3  \n",
    "4  17  0  \n",
    "\n",
    "[5 rows x 33 columns]\n",
    "\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fadc3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eb85e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368a1b5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d243c99c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ae6e1d",
   "metadata": {},
   "source": [
    "\n",
    "make a pairplot of the sentiment\tpositive\tneutral\tnegative, words in the post, and length of the post to see if there are any correlations between these variables.\n",
    "\n",
    "This graphic has several interesting key elements.\n",
    "\n",
    "We now have several columns that we don't want to interfere with the analysis as they could be considered \"leaks\" of information. We will drop these columns and save the data to a new csv file.\n",
    "These are: \n",
    "1. `author`\n",
    "2. `created_utc`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21445f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "top_authors_ocd.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e10ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "top_authors_autism.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e855de",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# are there any authors that are in both dataframes?\n",
    "print(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')\n",
    "list_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))\n",
    "print(f'List of authors that are in both dataframes: {list_of_cross_posters}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# drop author_flair_type and author_fullname columns from both dataframes\n",
    "df_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "df_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "# combine the title and self text columns into one column with the format `title - selftext`\n",
    "df_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\n",
    "df_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n",
    "# drop the title and selftext columns\n",
    "df_ocd = df_ocd.drop(columns=['title', 'selftext'])\n",
    "df_autism = df_autism.drop(columns=['title', 'selftext'])\n",
    "\n",
    "# rename the `title_selftext` column to `selftext`\n",
    "df_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\n",
    "df_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n",
    "\n",
    "cancel_words = [' ocd ',' aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']\n",
    "\n",
    "# remove punctuation\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n",
    "# remove whitespace\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "# do the same for the autism dataframe\n",
    "df_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n",
    "# remove punctuation\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n",
    "# remove whitespace\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n",
    "\n",
    "# then remove double spaces\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n",
    "\n",
    "# make a new dataframe called df_reddit that combines the two dataframes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a195d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# randomly sample one post from each dataframe and print it\n",
    "print(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\n",
    "print('='*100)\n",
    "print(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# randomly sample one post from each dataframe and print it\n",
    "print(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\n",
    "print('='*100)\n",
    "print(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992add0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "df_reddit = pd.DataFrame(columns=df_ocd.columns)\n",
    "# what is the length of the shorter dataframe?\n",
    "if len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n",
    "    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n",
    "    longer_df = df_autism # set the longer dataframe to the Autism dataframe\n",
    "else: # if the Autism dataframe is shorter\n",
    "    shorter_df = df_autism\n",
    "    longer_df = df_ocd\n",
    "\n",
    "# add the shorter dataframe to the new dataframe using concat\n",
    "df_reddit = pd.concat([df_reddit, shorter_df], axis=0)\n",
    "# shorten the longer dataframe to the length of the shorter dataframe\n",
    "longer_df = longer_df.head(len(shorter_df))\n",
    "# add the shortened longer dataframe to the new dataframe using concat\n",
    "df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n",
    "\n",
    "# reset the index\n",
    "df_reddit = df_reddit.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# shuffle the dataframe\n",
    "df_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n",
    "# check the dimensions of the new dataframe\n",
    "print(f'Dimensions of the new dataframe: {df_reddit.shape}')\n",
    "df_reddit.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# double check that the number of posts for each subreddit is the same\n",
    "print(f'Number of posts for OCD: {len(df_reddit[df_reddit.target == 1])}')\n",
    "print(f'Number of posts for Autism: {len(df_reddit[df_reddit.target == 0])}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab210d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "df_ocd.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34737eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "df_autism.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80164d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# find any of the medications in the selftext column that are in the data/drug_info.csv file under the Medication Name column and replace them with ' ' (empty string)\n",
    "drug_info = pd.read_csv('../data/drug_info.csv')\n",
    "drug_info['Medication Name'] = drug_info['Medication Name'].str.lower()\n",
    "# create a list of the medications\n",
    "medications = drug_info['Medication Name'].tolist()\n",
    "print(f'Number of medications: {len(medications)}')\n",
    "# how many posts contain a medication?\n",
    "print(f'Number of posts that contain a medication: {len(df_reddit[df_reddit.selftext.str.contains(\"|\".join(medications), regex=True)])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "medications[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "medications = [med for med in medications if len(med) > 5]\n",
    "# create a list of rows and the medications mentioned in each row\n",
    "import os\n",
    "medications_mentioned = []\n",
    "if os.path.exists('../data/cleaned_reddit.csv'):\n",
    "    pass\n",
    "else:\n",
    "    # with alive_bar (len(df_reddit)) as bar:\n",
    "    for index, row in df_reddit.iterrows(): # iterate through each row in the dataframe\n",
    "        # use regex to find all of the medications in the selftext column\n",
    "        meds = re.findall(r'\\b(?:{})\\b'.format('|'.join(medications)), row['selftext'])\n",
    "        if len(meds) > 0: # if there are medications mentioned in the post\n",
    "            # replace the medications with ' ' (empty string)\n",
    "            row['selftext'] = re.sub(r'\\b(?:{})\\b'.format('|'.join(medications)), ' ', row['selftext'])\n",
    "            medications_mentioned.extend(meds) # add the medications to the medications_mentioned list\n",
    "            # remove duplicate medications\n",
    "            medications_mentioned = list(set(medications_mentioned))\n",
    "            # bar()\n",
    "# remove the words from the selftext column that are in the medications list\n",
    "# if the file does not already exist, create it\n",
    "if os.path.exists('../data/cleaned_reddit.csv'):\n",
    "    # load the file\n",
    "    df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "else:\n",
    "    print('File does not exist. Creating it now. Before meds removed from selftext the length of the dataframe is: ', len(df_reddit))\n",
    "    print(f' Removed {len(medications_mentioned)} medications from the selftext column')\n",
    "    # save the dataframe to a csv file\n",
    "    df_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n",
    "# Now we want to clean the text in the self text column\n",
    "# remove punctuation\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\d+', '')\n",
    "# remove double spaces\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'  ', ' ')\n",
    "# remove single characters\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "# remove newlines\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n",
    "# remove urls\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n",
    "# remove html tags\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n",
    "# remove extra spaces\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n",
    "# remove extra spaces at the beginning of the string\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n",
    "# remove extra spaces at the end of the string\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n",
    "\n",
    "# save progress to a csv file\n",
    "df_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n",
    "\n",
    "\n",
    "# read the file into a dataframe\n",
    "df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "# remove any rows that have a null value in the selftext column\n",
    "df_reddit = df_reddit.dropna(subset=['selftext'])\n",
    "# reset the index\n",
    "df_reddit = df_reddit.reset_index(drop=True)\n",
    "# check the dimensions of the dataframe\n",
    "print(f'Dimensions of the dataframe: {df_reddit.shape}')\n",
    "\n",
    "\n",
    "def num_distinct_words(text,df):\n",
    "    \"\"\"\n",
    "    num_distinct_words takes in a string and a dataframe and returns the number of distinct words in the string\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    :param text: string\n",
    "    :type text: str\n",
    "    :param df: dataframe\n",
    "    :type df: pandas.core.frame.DataFrame\n",
    "    :return: number of distinct words in the string\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    # for this text, find the words that do not appear in any other text in the dataframe column 'selftext'\n",
    "    # split the text into a list of words\n",
    "    if type(text) == str:\n",
    "        text = text.split(' ')\n",
    "        # find the number of words that are not in any other text in the dataframe\n",
    "    else:\n",
    "        # the text is a list of words\n",
    "        words = text\n",
    "        pass\n",
    "    #words = text.split(' ')\n",
    "    # find the number of words that are not in any other text in the dataframe\n",
    "    distinct_words = [word for word in words if word not in df['selftext'].str.split(' ').sum()]\n",
    "    number_distinct_words = len(distinct_words) # find the number of distinct words\n",
    "    return number_distinct_words, distinct_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f71389",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "df_reddit.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22608e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# save df_reddit to a csv file\n",
    "print('Saving the dataframe to a csv file')\n",
    "df_reddit.to_csv('../data/cleaned_reddit_before_dataviz.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "try:\n",
    "    #~ Dropping Constant Value columns from the data ~#\n",
    "    df.drop(columns=['is_original_content'], inplace=True)\n",
    "    #~ Dropping duplicated selftext rows from the data ~#\n",
    "    print(f'Before dropping duplicates, the shape of the data is: {df.shape}')\n",
    "    preshape = df.shape[0]\n",
    "    df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "    print(f'After dropping duplicates, the shape of the data is: {df.shape}')\n",
    "    print(f'The number of rows dropped is: {preshape - df.shape[0]}')\n",
    "except Exception as e:\n",
    "    print(f'{e} - No duplicates to drop')\n",
    "# Create a new column that is the length of the selftext\n",
    "df['selftext_length'] = df['selftext'].str.len()\n",
    "\n",
    "# Create a new column that is the number of words in the selftext\n",
    "df['selftext_word_count'] = df['selftext'].str.split().str.len()\n",
    "\n",
    "# A column for each letter of the alphabet that is the number of times that letter appears in the selftext\n",
    "for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    df[f'{letter}'] = df['selftext'].str.count(letter)\n",
    "\n",
    "# save the data to a csv\n",
    "df.to_csv('../data/cleaned_reddit.csv', index=False)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# make a figure plotting letters against number of occurances in selftext for each selftext length bin. To avoid the ValueError \"ValueError: num must be 1 <= num <= 16, not 17\" the number of bins is set to 25 instead of 26 (the number of letters in the alphabet).\n",
    "# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n",
    "# for this code block ignore the IndexError\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(20,20), sharey=True, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "# the suptitle should not have so much space between it and the subplots\n",
    "# the x and y labels should be larger\n",
    "for i, letter in enumerate('abcdefghijklmnopqrstuvwxyz'):\n",
    "    try:\n",
    "        ax = axes[i//5, i%5]\n",
    "        ax.scatter(df['selftext_length'], df[f'{letter}'], alpha=0.5)\n",
    "        ax.set_title(letter)\n",
    "        ax.set_xlabel('Number of Occurances')\n",
    "        ax.set_ylabel('Selftext Length')\n",
    "    except IndexError:\n",
    "        pass\n",
    "fig.suptitle('Letter Occurances in Selftext by Selftext Length', fontsize=20, y=0.92)\n",
    "plt.savefig('../images/letter_histograms.png')\n",
    "plt.show();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# if the data/cleaned_reddit_withsentiment.csv file does not exist, run the following code to create it\n",
    "# use alivebar\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# if the file already exists, skip this code block and load the file\n",
    "# if the file exists but the length of the rows is not the same as the length of the rows in the cleaned_reddit.csv file, run the following code to create it\n",
    "if not os.path.exists('../data/cleaned_reddit_withsentiment.csv') or os.path.getsize('../data/cleaned_reddit_withsentiment.csv') != os.path.getsize('../data/cleaned_reddit.csv'):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    # if the file has a sentiment column, then use that value for each row in the sentiment column (we are just missing some rows)\n",
    "    # for each row in df, get the sentiment value from the sentiment column and add it to the row in the sentiment column for the new dataframe\n",
    "    # if the sentiment column does not exist, then use the sentiment value from the sentiment_analysis function\n",
    "    with alive_bar(len(df)) as bar:\n",
    "        for i, row in df.iterrows():\n",
    "            try:\n",
    "                # load compound, pos, neu, neg values from the sentiment columns in df\n",
    "                compound = row['sentiment']\n",
    "                pos = row['positive']\n",
    "                neu = row['neutral']\n",
    "                neg = row['negative']\n",
    "            except KeyError:\n",
    "                compound, pos, neu, neg = analyzer.polarity_scores(row['selftext'])['compound'], analyzer.polarity_scores(row['selftext'])['pos'], analyzer.polarity_scores(row['selftext'])['neu'], analyzer.polarity_scores(row['selftext'])['neg']\n",
    "            # add the sentiment values to the new dataframe\n",
    "            df.loc[i, 'sentiment'] = compound\n",
    "            df.loc[i, 'positive'] = pos\n",
    "            df.loc[i, 'neutral'] = neu\n",
    "            df.loc[i, 'negative'] = neg\n",
    "            \n",
    "            bar()\n",
    "    # save the data to a csv\n",
    "    df.to_csv('../data/cleaned_reddit_withsentiment.csv', index=False)\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
