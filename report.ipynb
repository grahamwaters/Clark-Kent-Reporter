{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6ce396",
   "metadata": {},
   "source": [
    "OCD vs. Autism (A Reddit Thread NLP Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2f77c",
   "metadata": {},
   "source": [
    "Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2735b00",
   "metadata": {},
   "source": [
    "A Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cfcfe",
   "metadata": {},
   "source": [
    "Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bea46",
   "metadata": {},
   "source": [
    "About the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd22744",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da65c54",
   "metadata": {},
   "source": [
    "Files Provided and their Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d689d01",
   "metadata": {},
   "source": [
    "Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aee6a0",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26eda2e",
   "metadata": {},
   "source": [
    "add 'I' to the list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518b2e5",
   "metadata": {},
   "source": [
    "dict of users and their most common word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7df2b",
   "metadata": {},
   "source": [
    "most common words are in user_df.index, go from most to least common, and pick the first one that is not in the stop words list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86eb11e",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bc3e0",
   "metadata": {},
   "source": [
    "Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90790f70",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72457dd7",
   "metadata": {},
   "source": [
    "Comments on Misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac566ff",
   "metadata": {},
   "source": [
    "Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f99b19",
   "metadata": {},
   "source": [
    "Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5ef9d",
   "metadata": {},
   "source": [
    "Works Cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e71ba",
   "metadata": {},
   "source": [
    "*A project by Graham Waters, 2022*\n",
    "\n",
    "![banner](./images/1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee5ab1",
   "metadata": {},
   "source": [
    "- [OCD vs. Autism (A Reddit Thread NLP Analysis)](#ocd-vs-autism-a-reddit-thread-nlp-analysis)\n",
    "- [Executive Summary](#executive-summary)\n",
    "- [A Table of Contents](#a-table-of-contents)\n",
    "- [Methods](#methods)\n",
    "- [About the API](#about-the-api)\n",
    "- [Data Collection](#data-collection)\n",
    "- [Files Provided and their Sequence](#files-provided-and-their-sequence)\n",
    "- [Model Files](#model-files)\n",
    "- [Data Cleaning](#data-cleaning)\n",
    "  - [Balancing the Data](#balancing-the-data)\n",
    "  - [User Top Word Analysis](#user-top-word-analysis)\n",
    "  - [Keyword Data Leakage](#keyword-data-leakage)\n",
    "  - [Steps for Data Cleaning in this Study](#steps-for-data-cleaning-in-this-study)\n",
    "- [Feature Engineering](#feature-engineering)\n",
    "- [Visualizing the Data](#visualizing-the-data)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Top 25 Users by Post Count in the `r/OCD` Thread](#top-25-users-by-post-count-in-the-rocd-thread)\n",
    "  - [Top 25 Users by Post Count in the `r/Autism` Thread](#top-25-users-by-post-count-in-the-rautism-thread)\n",
    "  - [Bigrams and Trigrams are great ways to examine text data as well.](#bigrams-and-trigrams-are-great-ways-to-examine-text-data-as-well)\n",
    "    - [Model 1.1. Logistic Regression](#model-11-logistic-regression)\n",
    "    - [Model 1.2. Adaboost](#model-12-adaboost)\n",
    "    - [Model 1.3 Decision Tree](#model-13-decision-tree)\n",
    "  - [Models using lemmatization](#models-using-lemmatization)\n",
    "- [Comments on Misclassification](#comments-on-misclassification)\n",
    "- [Conclusions and Recommendations](#conclusions-and-recommendations)\n",
    "- [Future Work](#future-work)\n",
    "- [Works Cited](#works-cited)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![](./images/3.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82910926",
   "metadata": {},
   "source": [
    "\n",
    "Pushshift's API is relatively straightforward. For example, if We want the posts from [`/r/boardgames`](https://www.reddit.com/r/boardgames), all We have to do is use the following URL: https://api.pushshift.io/reddit/search/submission?subreddit=boardgames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e42f4b",
   "metadata": {},
   "source": [
    "The files are ordered as follows:\n",
    "\n",
    "1. feature_engineering.ipynb # This file contains the code for the feature engineering process.\n",
    "2. data_cleaning.py # data cleaning functions that We consolidated from a data cleaning notebook for space optimization.\n",
    "3. data_exploration.ipynb # The exploration of the data.\n",
    "4. modeling.ipynb # the first iteration of modeling and analysis\n",
    "5. modeling_beta.ipynb # A streamlined version of modeling.ipynb with added lemmatized models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0cf6e",
   "metadata": {},
   "source": [
    "During the project's first iteration and early stages, scores were very high (around 0.98 R2) with minimal data cleaning. This revealed that there was multicollinearity or interdependence within the variables. We used for analysis. The following sections will illustrate the step-by-step process that led to the high-level data-cleaning decisions that we took to address these issues. One step we took was removing medication names from the r/OCD thread and any mention of the terms `OCD,` and `autism` (and their derivative terms) from posts used to train the classification models. This accounted for some of the overfitting in Our analysis, but further analysis would benefit from a more rigorous application of regex and data cleaning to identify new word patterns that could skew results one way or the other. Accuracy can be a misleading metric depending on the project's goals; we may want to optimize for something like an F1 score in the future instead of pure accuracy and comparison to baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a2db2",
   "metadata": {},
   "source": [
    "After removing stopwords, We used a combination of `nltk` and python's `re` library to generate a list of keywords for each user. We then used these keywords to create a dictionary of users and their most common keywords.\n",
    "I used this dictionary to filter the data and remove users that did not have at least ten posts.\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881214e4",
   "metadata": {},
   "source": [
    "users_aut = df_aut['author'].value_counts().index\n",
    "users_aut_most_common_word = {}\n",
    "for user in users_aut:\n",
    "user_df = df_aut[df_aut['author'] == user]\n",
    "user_df = user_df['selftext'].str.split(expand=True).stack().value_counts()[:50].sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f431ec",
   "metadata": {},
   "source": [
    "break # break out of the for loop\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66c273",
   "metadata": {},
   "source": [
    "1. Remove stopwords\n",
    "2. Remove Medications\n",
    "3. Combine title and selftext fields into one\n",
    "4. Remove users with fewer than ten posts\n",
    "5. Remove punctuation from the text to prepare it for the count vectorizer.\n",
    "\n",
    "\n",
    "What words are the most frequent unique words in each of the threads?\n",
    "Create a visual to analyze unique words count versus post length in the `r/Autism` thread to examine vocabulary density and diversity.\n",
    "Do the same for the `r/OCD` thread.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eeb9d8",
   "metadata": {},
   "source": [
    "\n",
    "I used a combination of `matplotlib` and `seaborn` to visualize the study's results. We wanted to see how the features were distributed and related to each other. What We was looking for was the presence of outliers and any other anomalies that would need to be addressed before modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bb738",
   "metadata": {},
   "source": [
    "\n",
    "![](./images/top_25_users_ocd_by_posts_with_word.png)\n",
    "\n",
    "This reveals that there are a large number of posts that have the author deleted, which could mean a deactivated account. This is not a problem for the model as it will not use the author's name as a feature. It would be helpful to remove the posts with the author deleted, though, as they are not valid for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c9cca",
   "metadata": {},
   "source": [
    "Bigrams are two words that are frequently used together. Trigrams are three words that are frequently used together. We can use these to examine the text and see if there are any patterns that we can use to improve the model.\n",
    "\n",
    "![](./images/top_25_bigrams_ocd.png)\n",
    "\n",
    "\n",
    "![](./images/top_25_trigrams_ocd.png)\n",
    "\n",
    "![](./images/top_25_bigrams_aut.png)\n",
    "\n",
    "![](./images/top_25_trigrams_aut.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdecb0",
   "metadata": {},
   "source": [
    "\n",
    "The second model we tested was an AdaBoost model that used logistic regression as its base estimator. Our parameters for the ADA boost model are shown below. We tested 50, 100, and 150 estimators with varying learning rates from .1 up to 1. We also instantiated a grid search on this Adaboost model with three-fold cross-validation, and the results of this model were interesting. The score on the training data was .95 or 95.3% accurate, while the score on the testing data was 91.8% accurate. This is an improvement of 41.8% over the baseline accuracy, but We am still interested to see what further testing can show.\n",
    "\n",
    "The best parameters were a learning rate of 1 with 150 estimators, 3000 Max features for the count vectorizer, and a `max_df` of 0.90. This model performed the best in the end, outperforming baseline accuracy by 41.8% and outperforming the logistic regression model by 0.50%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85be7b0",
   "metadata": {},
   "source": [
    "Results did not improve enough to warrant using lemmatization. Therefore, we will leave the code in the repository for reference but will not use it in the final model.\n",
    "The results for the lemmatized models are as follows:\n",
    "\n",
    "* The Adaboost model scored 89.1% on the testing set and 98.9% on the training set.\n",
    "* The logistic regression scored 80% on the testing and 99.4% on the training set.\n",
    "* The decision tree scored 88.9% on the testing and 90.2% on the training set.\n",
    "\n",
    "Ultimately the logistic regression model was best at classifying posts for these threads, as it could offer insight into inference, while the others tend to be more opaque. Overall, lemmatization did not seem to improve the scores on the testing set though it might have improved the quality of the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714153ca",
   "metadata": {},
   "source": [
    "Our initial hypothesis that a combination of certain NLP techniques, such as sentiment analysis and count vectorization, could be used to build a model that accurately can predict whether a post is from the Autism or OCD subreddit was not sufficiently proved by the study.\n",
    "\n",
    "Our Alpha Model was good at predicting which subreddit a post belonged to, with accuracy scores between 90 to 99% on the training data. It also scored extremely high on the test data, indicating that some features within the data were overpowering the model and causing it to overfit. One could stop there and be done with their analysis, saying that the model does technically predict which subreddit a post belongs to, but we wanted to see if we could improve the model for the client. For this reason, we moved to the next stage and created our Alpha Models.\n",
    "\n",
    "Our Beta Models could predict with 91.8% accuracy (on the test set) whether a post was from the Autism or OCD subreddit. This is a significant improvement over the baseline accuracy of 50%. However, it is still not good enough to be used in a production environment. The model is still overfitting, and We believe this is because the data is not clean enough. There are still many words that are irrelevant to the model, and We believe that if We were to clean the data further, We would be able to improve the model.\n",
    "\n",
    "Our final recommendations are that we number one gather more data and consistently measure these two subreddits to gain a more holistic understanding of what these populations enjoy, what they participate in, what kinds of verbs they use, or nouns they prefer. We would also like to explore one of the features we created that has to do with questions. How many users post questions versus discussions, and are these skewed towards one or the other forum?\n",
    "\n",
    "There are 8949 unique users in the data frame, and this is a large number when you consider the neuro diversity that exists not only on a spectrum but also on the side of OCD. This study opens many doors for future work and shows promising results though less in the area of linguistics and more in the area of simple prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "# add 'I' to the list of stopwords\n",
    "# dict of users and their most common word\n",
    "users_aut = df_aut['author'].value_counts().index\n",
    "users_aut_most_common_word = {}\n",
    "for user in users_aut:\n",
    "user_df = df_aut[df_aut['author'] == user]\n",
    "user_df = user_df['selftext'].str.split(expand=True).stack().value_counts()[:50].sort_values(ascending=True)\n",
    "# most common words are in user_df.index, go from most to least common, and pick the first one that is not in the stop words list\n",
    "for word in user_df.index:\n",
    "if word not in stopwords_list: # if the word is not in the stop words list\n",
    "users_aut_most_common_word[user] = word\n",
    "#print(f'{user} most common word: {word}')\n",
    "break # break out of the for loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "output\n",
    "Training score: 0.991991643454039\n",
    "Testing score: 0.9139972144846796\n",
    "Best score: 0.8992571959145775\n",
    "Best params: {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'logreg__C': 1, 'logreg__penalty': 'l2'}\n",
    "Best estimator: Pipeline(steps=[('cvec', CountVectorizer(max_df=0.9, max_features=3000, min_df=2)), ('logreg', LogisticRegression(C=1, solver='liblinear'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "output\n",
    "pipe_params_sent_len = {\n",
    "'cvec__max_features': [1000, 2000, 3000],\n",
    "'cvec__min_df': [2, 3],\n",
    "'cvec__max_df': [.9, .95],\n",
    "'cvec__ngram_range': [(1,1), (1,2)],\n",
    "'logreg__penalty': ['l1','l2'],\n",
    "'logreg__C': [1, 2, 3]\n",
    "}\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
